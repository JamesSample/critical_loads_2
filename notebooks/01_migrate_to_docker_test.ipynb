{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import nivapy3 as nivapy\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import critical_loads as cl\n",
    "import os\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critcal Loads: Dockerised workflow (November 2018)\n",
    "\n",
    "This notebook begins the process of transferring the Critical Loads workflow out of RESA and into Docker, so that it can eventually be moved onto the new data platform. A workflow is provided for creating a new PostGIS database in Docker and populating it with the key datasets required for the critical loads calculations. Some data exploration and checking is also presented.\n",
    "\n",
    "In brief, the workflow looks like this:\n",
    "\n",
    " 1. Create a local Docker container running PostGIS\n",
    " \n",
    " 2. Use the [Data Science Toolkit](https://github.com/NIVANorge/niva_datasci_toolkit) to transfer all relevant datasets into the new PostGIS database\n",
    " \n",
    " 3. Create an identical (empty) PostGIS container running on Kubernetes in Google Cloud\n",
    " \n",
    " 4. Use `pg_dump` to transfer all the data from the local container to the cloud container\n",
    " \n",
    " 5. Expose the cloud container to allow connections/queries from e.g. the Toolkit or QGIS\n",
    "\n",
    "## 1. Create a PostGIS database running on Docker\n",
    "\n",
    "**Update 23/11/2018:** I initially used the `kartoza/postgis:9.6-2.4` image for my testing, but it seems that `timescale/timescaledb-postgis:latest-pg10` is more compatible with our current setup on GCP. My original code for creating the PostGIS container was \n",
    "\n",
    "    # Create container\n",
    "    docker run --name=postgis_crit_lds -d -e POSTGRES_USER=admin -e POSTGRES_PASS=pw1234 -e POSTGRES_DBNAME=critical_loads -e ALLOW_IP_RANGE=0.0.0.0/0 -p 25432:5432 -v pg_data_crit_lds:/var/lib/postgresql kartoza/postgis:9.6-2.4\n",
    "    \n",
    "but this has now been changed in favour of the workflow described below.\n",
    "\n",
    "### 1.1. Create data volume\n",
    "\n",
    "The following code creates a data volume on the (local) Docker host.\n",
    "\n",
    "    # Create data volume\n",
    "    docker volume create pg_data_crit_lds\n",
    "\n",
    "### 1.2. Create container\n",
    "\n",
    "    # Create container\n",
    "    docker run -d --name postgis_crit_lds -p 25432:5432 -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=pw1234 -e POSTGRES_DB=critical_loads -e PGDATA=/var/lib/postgresql/data/pg10 -e POSTGIS_GDAL_ENABLED_DRIVERS=ENABLE_ALL -v pg_data_crit_lds:/var/lib/postgresql/data timescale/timescaledb-postgis:latest-pg10\n",
    "    \n",
    "**Note:** Once the container has been created, it can be stopped/started at any time using\n",
    "\n",
    "    # Start\n",
    "    docker start postgis_crit_lds\n",
    "    \n",
    "    # Stop\n",
    "    docker stop postgis_crit_lds\n",
    "    \n",
    "We can now start the DS Toolkit and establish connections to the following databases:\n",
    "\n",
    " 1. **RESA2**. A centralised NIVA database containing most of the old (non-spatial) data relating to the Critical Loads project\n",
    " \n",
    " 2. **PostGIS running on `localhost`**. My local PostGIS installation, which stores some of the new spatial datasets required for the updated calculations\n",
    " \n",
    " 3. **PostGIS running on Docker**. The new database created for the updated workflow. We wish to transfer all the Critical Loads datasets (both spatial and non-spatial) to this database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n"
     ]
    }
   ],
   "source": [
    "# Connect to the NIVABASE\n",
    "ora_eng = nivapy.da.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n"
     ]
    }
   ],
   "source": [
    "# Connect to local PostGIS\n",
    "loc_pg_eng = nivapy.da.connect(src='postgres',\n",
    "                               db_name='niva_work',\n",
    "                               port=5432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n"
     ]
    }
   ],
   "source": [
    "# Connect to Docker PostGIS\n",
    "doc_pg_eng = nivapy.da.connect(src='postgres',\n",
    "                               db_name='critical_loads',\n",
    "                               port = 25432)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Activate the PostGIS extension\n",
    "\n",
    "**Note:** This step is required for the `timescale/timescaledb-postgis` container, but *not* for the `kartoza/postgis:9.6-2.4` container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f90506c96a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Activate PostGIS extension\n",
    "sql = \"CREATE EXTENSION IF NOT EXISTS postgis\"\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deposition data\n",
    "\n",
    "First, create a new schema to store deposition data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f905060cf98>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create deposition schema\n",
    "sql = \"CREATE SCHEMA deposition\"\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Transfer BLR and 0.1 degree deposition grids\n",
    "\n",
    "Deposition data from NILU is now supplied on a 0.1 degree grid, which is currently stored in my local PostGIS installation. The code below transfers it to the new schema in Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f902b79f518>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 0.1 degree dep grid from PostGIS\n",
    "sql = (\"SELECT * FROM public.dep_grid_0_1deg\")\n",
    "dep_gdf = gpd.read_postgis(sql, loc_pg_eng)\n",
    "\n",
    "# Write to new db\n",
    "nivapy.da.gdf_to_postgis(dep_gdf, \n",
    "                         'dep_grid_0_1deg', \n",
    "                         'deposition', \n",
    "                         doc_pg_eng,\n",
    "                         'dep_dep_grid_0_1deg_spidx',\n",
    "                         if_exists='replace',\n",
    "                         index=False)\n",
    "\n",
    "# Use 'cell_id' col as primary key\n",
    "sql = (\"ALTER TABLE deposition.dep_grid_0_1deg \"\n",
    "       \"ADD CONSTRAINT dep_grid_0_1deg_pk \"\n",
    "       \"PRIMARY KEY (cell_id)\")\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous years, data have been supplied using the BLR grid, which is available as a shapefile. The code below loads this into the database as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! The dataframe contains mixed geometries:\n",
      "  {'Polygon', 'MultiPolygon'}\n",
      "These will be cast to \"Multi\" type. If this is not what you want, consider using gdf.explode() first\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f90018cce10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read BLR .shp\n",
    "blr_path = (r'../../../data/vector/blrgrid_uten_grums.shp')\n",
    "dep_gdf = gpd.read_file(blr_path)\n",
    "dep_gdf.rename({'BLR':'blr'},\n",
    "               axis=1,\n",
    "               inplace=True)\n",
    "\n",
    "# Write to new db\n",
    "nivapy.da.gdf_to_postgis(dep_gdf, \n",
    "                         'dep_grid_blr', \n",
    "                         'deposition', \n",
    "                         doc_pg_eng,\n",
    "                         'dep_dep_grid_blr_spidx',\n",
    "                         if_exists='replace',\n",
    "                         index=False)\n",
    "\n",
    "# Use 'blr' col as primary key\n",
    "sql = (\"ALTER TABLE deposition.dep_grid_blr \"\n",
    "       \"ADD CONSTRAINT dep_grid_blr_pk \"\n",
    "       \"PRIMARY KEY (blr)\")\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Transfer \"Deposition Series Definitions\" from the NIVABASE\n",
    "\n",
    "In RESA2, the table `DEP_SERIES_DEFINITIONS` identifies the datasets previously supplied by NILU. Each dataset has a separate row (and ID) in this table. For consistency, I will transfer this table directly and use the same dataset IDs in the new workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24</td>\n",
       "      <td>expost gridav MFR2020-Update</td>\n",
       "      <td>Lagt inn i forb. med ex-Post analyser 2010 For...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25</td>\n",
       "      <td>Middel 2007-2011</td>\n",
       "      <td>Fordelt til BLR av NILU 2012 (Wenche Aas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>26</td>\n",
       "      <td>Middel 2012-2016</td>\n",
       "      <td>Fordelt til BLR av NILU 2017 (Wenche Aas; old ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>27</td>\n",
       "      <td>Middel 2012-2016 (new)</td>\n",
       "      <td>Fordelt til BLR av NILU 2017 (Wenche Aas; new ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>28</td>\n",
       "      <td>Middel 2012-2016 (new; hi-res)</td>\n",
       "      <td>Fordelt til BLR av NILU 2017 (Wenche Aas; new ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    series_id                            name  \\\n",
       "14         24    expost gridav MFR2020-Update   \n",
       "15         25                Middel 2007-2011   \n",
       "27         26                Middel 2012-2016   \n",
       "16         27          Middel 2012-2016 (new)   \n",
       "17         28  Middel 2012-2016 (new; hi-res)   \n",
       "\n",
       "                                          description  \n",
       "14  Lagt inn i forb. med ex-Post analyser 2010 For...  \n",
       "15          Fordelt til BLR av NILU 2012 (Wenche Aas)  \n",
       "27  Fordelt til BLR av NILU 2017 (Wenche Aas; old ...  \n",
       "16  Fordelt til BLR av NILU 2017 (Wenche Aas; new ...  \n",
       "17  Fordelt til BLR av NILU 2017 (Wenche Aas; new ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from RESA\n",
    "sql = \"SELECT * FROM resa2.dep_series_definitions\"\n",
    "df = pd.read_sql(sql, ora_eng)\n",
    "\n",
    "# Tidy\n",
    "df.rename({'dep_series_id':'series_id'},\n",
    "          axis=1,\n",
    "          inplace=True)\n",
    "df.sort_values('series_id', inplace=True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f90018ccc50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write to new db\n",
    "df.to_sql('dep_series_defs', \n",
    "          doc_pg_eng,\n",
    "          'deposition',\n",
    "          if_exists='replace',\n",
    "          index=False)\n",
    "\n",
    "# Use 'dep_series_id' col as primary key\n",
    "sql = (\"ALTER TABLE deposition.dep_series_defs \"\n",
    "       \"ADD CONSTRAINT dep_series_defs_pk \"\n",
    "       \"PRIMARY KEY (series_id)\")\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Transfer \"Deposition Parameter Definitions\" from the NIVABASE\n",
    "\n",
    "In RESA2, the table `AIR_PARAMETER_DEFINITIONS` identifies various deposition parameters. As far as I can see, only the first three or four entries in this table are currently relevant. Many of the columns also seem unnecessary at present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_id</th>\n",
       "      <th>name</th>\n",
       "      <th>unit</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>N(oks)</td>\n",
       "      <td>mg N/m2/year</td>\n",
       "      <td>Oxidized nitrogen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>N(red)</td>\n",
       "      <td>mg N/m2/year</td>\n",
       "      <td>Reduced nitogen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>S</td>\n",
       "      <td>mg S/m2/year</td>\n",
       "      <td>Total sulphur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>S*</td>\n",
       "      <td>mg S/m2/year</td>\n",
       "      <td>Non marine sulphur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_id    name          unit          description\n",
       "0         1  N(oks)  mg N/m2/year    Oxidized nitrogen\n",
       "1         2  N(red)  mg N/m2/year      Reduced nitogen\n",
       "2         3       S  mg S/m2/year        Total sulphur\n",
       "3         4      S*  mg S/m2/year  Non marine sulphur "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from RESA\n",
    "sql = (\"SELECT * FROM resa2.air_parameter_definitions \"\n",
    "       \"WHERE parameter_id < 5\")\n",
    "df = pd.read_sql(sql, ora_eng)\n",
    "\n",
    "# Tidy\n",
    "df.sort_values('parameter_id', inplace=True)\n",
    "df.drop(['formula', 'category', 'function', 'entered_by', 'entered_date'],\n",
    "        axis=1,\n",
    "        inplace=True)\n",
    "df.rename({'parameter_id':'param_id'}, \n",
    "          inplace=True,\n",
    "          axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f90014ee198>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write to new db\n",
    "df.to_sql('dep_param_defs', \n",
    "          doc_pg_eng,\n",
    "          'deposition',\n",
    "          if_exists='replace',\n",
    "          index=False)\n",
    "\n",
    "# Use 'dep_series_id' col as primary key\n",
    "sql = (\"ALTER TABLE deposition.dep_param_defs \"\n",
    "       \"ADD CONSTRAINT dep_param_defs_pk \"\n",
    "       \"PRIMARY KEY (param_id)\")\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Data for \"Deposition Values\"\n",
    "\n",
    "#### 2.4.1. Create tables\n",
    "\n",
    "The old workflow used the coarse BLR grid for deposition values. Deposition `series_ids` from 1 to 27 are all based on this grid, so these values will be transferred to a new table named `'dep_values_blr_grid'`. Deposition `series_id=28` uses the new 0.1 degree grid, so this dataset (and subsequent ones) will need adding to a separate table (`'dep_values_0_1deg_grid'`). As future data will also be delivered using this grid, it will be useful to have a function for processing and adding raw data to this table.\n",
    "\n",
    "Additionally, both these tables need constraints to ensure that relevant series and parameter IDs are defined before uploading data. Note that I have not included a constraint on the BLR/cell ID, because the data supplied by NILU often includes cells that are outside of terrestrial Norway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f90014c9128>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete if already exist\n",
    "sql = (\"DROP TABLE IF EXISTS deposition.dep_values_blr_grid, \"\n",
    "       \"  deposition.dep_values_0_1deg_grid\")\n",
    "doc_pg_eng.execute(sql)\n",
    "\n",
    "# Create table for BLR data\n",
    "sql = (\"CREATE TABLE deposition.dep_values_blr_grid \"\n",
    "       \"( \"\n",
    "       \"  series_id integer NOT NULL, \"\n",
    "       \"  blr integer NOT NULL, \"\n",
    "       \"  param_id integer NOT NULL, \"\n",
    "       \"  value numeric, \"\n",
    "       \"  PRIMARY KEY (series_id, blr, param_id), \"\n",
    "       \"  CONSTRAINT series_id_fkey FOREIGN KEY (series_id) \"\n",
    "       \"      REFERENCES deposition.dep_series_defs (series_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION, \"\n",
    "       \"  CONSTRAINT param_id_fkey FOREIGN KEY (param_id) \"\n",
    "       \"      REFERENCES deposition.dep_param_defs (param_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION \"\n",
    "       \")\")\n",
    "doc_pg_eng.execute(sql)\n",
    "\n",
    "# Create table for 0.1 degree data\n",
    "sql = (\"CREATE TABLE deposition.dep_values_0_1deg_grid \"\n",
    "       \"( \"\n",
    "       \"  series_id integer NOT NULL, \"\n",
    "       \"  cell_id integer NOT NULL, \"\n",
    "       \"  param_id integer NOT NULL, \"\n",
    "       \"  value numeric, \"\n",
    "       \"  PRIMARY KEY (series_id, cell_id, param_id), \"\n",
    "       \"  CONSTRAINT series_id_fkey FOREIGN KEY (series_id) \"\n",
    "       \"      REFERENCES deposition.dep_series_defs (series_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION, \"\n",
    "       \"  CONSTRAINT param_id_fkey FOREIGN KEY (param_id) \"\n",
    "       \"      REFERENCES deposition.dep_param_defs (param_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION \"\n",
    "       \")\")\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2. Transfer \"old\" data\n",
    "\n",
    "The old data using the BLR grid are stored in RESA2 in the table `DEP_BLR_VALUES`. We'll just transfer the data for `param_ids` 1 to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from RESA\n",
    "sql = (\"SELECT * FROM resa2.dep_blr_values \"\n",
    "       \"WHERE parameter_id < 5\")\n",
    "df = pd.read_sql(sql, ora_eng)\n",
    "\n",
    "# Tidy\n",
    "df.rename({'dep_series_id':'series_id',\n",
    "           'parameter_id':'param_id'},\n",
    "          axis=1,\n",
    "          inplace=True)\n",
    "\n",
    "# Write to new db\n",
    "df.to_sql('dep_values_blr_grid', \n",
    "          doc_pg_eng,\n",
    "          'deposition',\n",
    "          if_exists='append',\n",
    "          index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3. Import \"new\" data\n",
    "\n",
    "In 2017, NILU supplied raw data for the 0.1 degree grid in `.dat` format. I previously wrote some code ([here](http://nbviewer.jupyter.org/github/JamesSample/critical_loads/blob/master/notebooks/critical_loads_workflow_new_grid.ipynb#1.1.-Upload-new-data-to-database)) to process this data. I have now generalised this and moved it into a new `.py` file, which will eventually become a Python package/module for the Critical Loads workflow.\n",
    "\n",
    "**Note:** Double-check with Kari that the NILU file `tot_s_combined-emep_grid.dat` contains data for *non-marine* S. We assumed this in the 2018 analysis, but worth checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207000 new rows added successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>param_id</th>\n",
       "      <th>value</th>\n",
       "      <th>series_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50050305</td>\n",
       "      <td>2</td>\n",
       "      <td>270.87</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50050315</td>\n",
       "      <td>2</td>\n",
       "      <td>240.50</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50050325</td>\n",
       "      <td>2</td>\n",
       "      <td>259.19</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50050335</td>\n",
       "      <td>2</td>\n",
       "      <td>252.06</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50050345</td>\n",
       "      <td>2</td>\n",
       "      <td>332.82</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cell_id param_id   value  series_id\n",
       "0  50050305        2  270.87         28\n",
       "1  50050315        2  240.50         28\n",
       "2  50050325        2  259.19         28\n",
       "3  50050335        2  252.06         28\n",
       "4  50050345        2  332.82         28"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process NILU data and add to db\n",
    "nilu_fold = r'../../../data/raw/nilu_dep/2012-2016'\n",
    "df = cl.upload_nilu_0_1deg_dep_data(nilu_fold, \n",
    "                                    doc_pg_eng,\n",
    "                                    28)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vegatation data\n",
    "\n",
    "First, create a new schema for data relating to the vegetation calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f902b713898>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create veg schema\n",
    "sql = \"CREATE SCHEMA vegetation\"\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Transfer critical loads table for vegetation\n",
    "\n",
    "The Excel file \n",
    "\n",
    "    new_workflow_nov_2018\\data\\raw\\veg_cl_classes\\sat_veg_land_use_classes.xlsx\n",
    "    \n",
    "contains critical load values for various vegetation classes. The original is in the sheet named `'EUNIS_tilGIS'`, but I have also created a tidied version (with e.g. lower case column names) in the sheet named `'eunis_lower_case'`.\n",
    "\n",
    "As above, if this table is likely to change, it should be normalised and loaded into the database more carefully. For now, the \"flat\" Excel format is convenient, but I nevertheless want to store it in the database with the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f900150b630>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Excel data\n",
    "xl_path = r'../../../data/raw/veg_cl_classes/sat_veg_land_use_classes.xlsx'\n",
    "df = pd.read_excel(xl_path, sheet_name='eunis_lower_case')\n",
    "\n",
    "# Write to new db\n",
    "df.to_sql('land_class_crit_lds', \n",
    "          doc_pg_eng,\n",
    "          'vegetation',\n",
    "          if_exists='replace',\n",
    "          index=False)\n",
    "\n",
    "# Use 'norut_code' as primary key\n",
    "sql = (\"ALTER TABLE vegetation.land_class_crit_lds \"\n",
    "       \"ADD CONSTRAINT veg_land_class_crit_lds_pk \"\n",
    "       \"PRIMARY KEY (norut_code)\")\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Transfer vector vegetation data\n",
    "\n",
    "Analyses undertaken for the periods 2002-2006 and 2007-2011 used a vector vegetation dataset. From 2012-2016 onwards, this has been replaced by a more detailed raster dataset (Section 3.3, below). The code in this section adds the old vector dataset to PostGIS for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f9001167438>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read veg .shp\n",
    "shp_path = (r'../../../data/vector/nye_2010_talegrenser_veg.shp')\n",
    "veg_gdf = gpd.read_file(shp_path)\n",
    "\n",
    "# Tidy\n",
    "del veg_gdf['ID_1'], veg_gdf['GRIDCODE'], veg_gdf['EUNIS_1']\n",
    "veg_gdf.columns = [i.lower() for i in veg_gdf.columns]\n",
    "\n",
    "# Reproject to WGS84\n",
    "veg_gdf = veg_gdf.to_crs({'init':'epsg:4326'})\n",
    "\n",
    "# Write to new db\n",
    "nivapy.da.gdf_to_postgis(veg_gdf, \n",
    "                         'vector_veg_pre_2012', \n",
    "                         'vegetation', \n",
    "                         doc_pg_eng,\n",
    "                         'vector_veg_pre_2012_spidx',\n",
    "                         if_exists='replace',\n",
    "                         index=False)\n",
    "\n",
    "# Use 'id' col as primary key\n",
    "sql = (\"ALTER TABLE vegetation.vector_veg_pre_2012 \"\n",
    "       \"ADD CONSTRAINT vector_veg_pre_2012_pk \"\n",
    "       \"PRIMARY KEY (id)\")\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Transfer vegetation grids\n",
    "\n",
    "The code in Section 1.3 of [this notebook](http://nbviewer.jupyter.org/github/JamesSample/critical_loads/blob/master/notebooks/critical_loads_workflow_new_grid.ipynb#1.3.-Reclassify) converted 30 m resolution satellite vegetation data into a grid of critical loads (based on the Excel table above - Section 3.1). Unless we update the vegetation data, this calculation does not need to be performed again, but it would be useful to store the resulting raster grid (`sat_veg_30m_cr_lds_div100.tif`) and the original vegetation grid (`sat_veg_30m_all.tif`) in the PostGIS database. As GeoTiffs, these grids are large: `sat_veg_30m_cr_lds_div100.tif` is around 2.3 GB uncompressed. PostGIS compresses the grids when they are uploaded, and it is possible to get the final raster size in the database using the folowing SQL:\n",
    "\n",
    "    SELECT pg_size_pretty(pg_total_relation_size('vegetation.sat_veg_30m_cr_lds_div100'));\n",
    "    \n",
    "This query returns 121 MB i.e. a vast reduction in file size. This is great for data storage, but terrible for computational performance, because the raster needs to be uncompressed each time before it can be used. For any data processing, it is therefore *much* faster to store the grids locally as uncompressed GeoTiff files. In the data analysis workflow, I will therefore include functions for exporting grids from PostGIS to GeoTiff format. This step only needs to be done once - all subsequent steps can then make use of the local GeoTiff files.\n",
    "\n",
    "**Note:** It is possible to load raster data into PostGIS using NivaPy (via `raster2pgsql`). For reference, a brief explanation of the underlying command systax is given below. \n",
    " \n",
    "         raster2pgsql -s 32633 -d -C -I -M -l 2,4,8,16,32,64,128,256,512 C:\\Data\\James_Work\\Staff\\Kari_A\\Critical_Loads\\GIS\\raster\\sat_veg_30m_cr_lds_div100.tif -t 100x100 vegetation.sat_veg_30m_cr_lds_div100 | psql -h localhost -U postgres -p 25432 -d critical_loads\n",
    "    \n",
    "    <br>You will be prompted to enter the password for the Docker database. This command specifies the following options (more details [here](https://postgis.net/docs/using_raster_dataman.html)):\n",
    "    \n",
    "     * **-s**. EPSG code for spatial reference\n",
    "     * **-d**. Drop ratser if already exists in database\n",
    "     * **-C**. Apply raster constraints -- srid, pixelsize etc. to ensure raster is properly registered\n",
    "     * **-I**. Build a spatial index\n",
    "     * **-M**. Vaccum analyze after loading\n",
    "     * **-l**. Build pyramids (9 levels)\n",
    "     * **-t**. Cut raster into tiles to be inserted one per table row\n",
    " \n",
    "    <br>The part after `psql` then specifies the connection details for the Docker database.  \n",
    "    \n",
    "For the vegetation exceedance calculations, the code below uploads the following grids:\n",
    "\n",
    " * `sat_veg_30m_all.tif`\n",
    " * `sat_veg_60m_all.tif`\n",
    " * `sat_veg_120m_all.tif`\n",
    " * `sat_veg_30m_cr_lds_div100.tif`\n",
    " * `sat_veg_60m_cr_lds_div100.tif`\n",
    " * `blr_land_mask.tif`\n",
    " * `blr_land_mask_60m.tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for vegetation.sat_veg_30m_all...\n",
      "\n",
      "Raster loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Add 30m veg to PostGIS\n",
    "nivapy.da.raster_to_postgis('../../../data/raster/sat_veg_30m_all.tif',\n",
    "                            'critical_loads',\n",
    "                            'vegetation',\n",
    "                            'sat_veg_30m_all', \n",
    "                            32633)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for vegetation.sat_veg_60m_all...\n",
      "\n",
      "Raster loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Add 60m veg to PostGIS\n",
    "nivapy.da.raster_to_postgis('../../../data/raster/sat_veg_60m_all.tif',\n",
    "                            'critical_loads',\n",
    "                            'vegetation',\n",
    "                            'sat_veg_60m_all', \n",
    "                            32633)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for vegetation.sat_veg_120m_all...\n",
      "\n",
      "Raster loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Add 120m veg to PostGIS\n",
    "nivapy.da.raster_to_postgis('../../../data/raster/sat_veg_120m_all.tif',\n",
    "                            'critical_loads',\n",
    "                            'vegetation',\n",
    "                            'sat_veg_120m_all', \n",
    "                            32633)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for vegetation.sat_veg_30m_cr_lds_div100...\n",
      "\n",
      "Raster loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Add 30m CL data to PostGIS\n",
    "nivapy.da.raster_to_postgis('../../../data/raster/sat_veg_30m_cr_lds_div100.tif',\n",
    "                            'critical_loads',\n",
    "                            'vegetation',\n",
    "                            'sat_veg_30m_cr_lds_div100', \n",
    "                            32633)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for vegetation.sat_veg_60m_cr_lds_div100...\n",
      "\n",
      "Raster loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Add 60m CL data to PostGIS\n",
    "nivapy.da.raster_to_postgis('../../../data/raster/sat_veg_60m_cr_lds_div100.tif',\n",
    "                            'critical_loads',\n",
    "                            'vegetation',\n",
    "                            'sat_veg_60m_cr_lds_div100', \n",
    "                            32633)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for vegetation.blr_land_mask_60m...\n",
      "\n",
      "Raster loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Add 60m land mask to PostGIS\n",
    "nivapy.da.raster_to_postgis('../../../data/raster/blr_land_mask_60m.tif',\n",
    "                            'critical_loads',\n",
    "                            'vegetation',\n",
    "                            'blr_land_mask_60m', \n",
    "                            32633)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for vegetation.blr_land_mask_30m...\n",
      "\n",
      "Raster loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Add 30m land mask to PostGIS\n",
    "nivapy.da.raster_to_postgis('../../../data/raster/blr_land_mask.tif',\n",
    "                            'critical_loads',\n",
    "                            'vegetation',\n",
    "                            'blr_land_mask_30m', \n",
    "                            32633)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Create tables to store exceedances\n",
    "\n",
    "Summary exceedance statistics for each grid cell are stored in the database in the following tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f90011d4668>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete if already exist\n",
    "sql = (\"DROP TABLE IF EXISTS vegetation.exceedance_stats_blr_grid, \"\n",
    "       \"  vegetation.exceedance_stats_0_1deg_grid\")\n",
    "doc_pg_eng.execute(sql)\n",
    "\n",
    "# Create table for BLR data\n",
    "sql = (\"CREATE TABLE vegetation.exceedance_stats_blr_grid \"\n",
    "       \"( \"\n",
    "       \"  series_id integer NOT NULL, \"\n",
    "       \"  blr integer NOT NULL, \"\n",
    "       \"  exceeded_area_km2 numeric, \"\n",
    "       \"  total_area_km2 numeric, \"\n",
    "       \"  pct_exceeded numeric, \"\n",
    "       \"  PRIMARY KEY (series_id, blr), \"\n",
    "       \"  CONSTRAINT series_id_fkey FOREIGN KEY (series_id) \"\n",
    "       \"      REFERENCES deposition.dep_series_defs (series_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION, \"\n",
    "       \"  CONSTRAINT blr_fkey FOREIGN KEY (blr) \"\n",
    "       \"      REFERENCES deposition.dep_grid_blr (blr) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION \"\n",
    "       \")\")\n",
    "doc_pg_eng.execute(sql)\n",
    "\n",
    "# Create table for BLR data\n",
    "sql = (\"CREATE TABLE vegetation.exceedance_stats_0_1deg_grid \"\n",
    "       \"( \"\n",
    "       \"  series_id integer NOT NULL, \"\n",
    "       \"  cell_id integer NOT NULL, \"\n",
    "       \"  exceeded_area_km2 numeric, \"\n",
    "       \"  total_area_km2 numeric, \"\n",
    "       \"  pct_exceeded numeric, \"\n",
    "       \"  PRIMARY KEY (series_id, cell_id), \"\n",
    "       \"  CONSTRAINT series_id_fkey FOREIGN KEY (series_id) \"\n",
    "       \"      REFERENCES deposition.dep_series_defs (series_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION, \"\n",
    "       \"  CONSTRAINT cell_id_fkey FOREIGN KEY (cell_id) \"\n",
    "       \"      REFERENCES deposition.dep_grid_0_1deg (cell_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION \"\n",
    "       \")\")\n",
    "doc_pg_eng.execute(sql)\n",
    "\n",
    "# Create table for exceedance per land class data\n",
    "sql = (\"CREATE TABLE vegetation.exceedance_stats_land_class \"\n",
    "       \"( \"\n",
    "       \"  series_id integer NOT NULL, \"\n",
    "       \"  norut_code integer NOT NULL, \"\n",
    "       \"  exceeded_area_km2 numeric, \"\n",
    "       \"  total_area_km2 numeric, \"\n",
    "       \"  pct_exceeded numeric, \"\n",
    "       \"  PRIMARY KEY (series_id, norut_code), \"\n",
    "       \"  CONSTRAINT series_id_fkey FOREIGN KEY (series_id) \"\n",
    "       \"      REFERENCES deposition.dep_series_defs (series_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION, \"\n",
    "       \"  CONSTRAINT norut_code_fkey FOREIGN KEY (norut_code) \"\n",
    "       \"      REFERENCES vegetation.land_class_crit_lds (norut_code) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION \"\n",
    "       \")\")\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Water data\n",
    "\n",
    "First, create a new schema for data relating to the water calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create water schema\n",
    "sql = \"CREATE SCHEMA water\"\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. MAGIC model output for regression\n",
    "\n",
    "Kari has a spreadsheet here\n",
    "\n",
    "    K:\\Avdeling\\317 Klima- og miljømodellering\\KAU\\Focal Centre\\Data\\bc0regresjonNOK_TL2005-rapport_KAU.xls\n",
    "    \n",
    "containing output from the MAGIC model. This is used to generate parameters for the calculation of critical loads to water. It is unclear whether this data will be updated in the future. If so, the dataset should be properly normalised before adding to the database. However, for now I'm assuming that this is a static dataset, so I'm just athe essential data to the database as a \"flat\" table for the purposes of data storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tidied Excel data\n",
    "xl_path = r'../../../../water/update_nov_2018/regression_data_tidied.xls'\n",
    "df = pd.read_excel(xl_path, sheet_name='tidied')\n",
    "\n",
    "# Write to new db\n",
    "df.to_sql('magic_regression_data', \n",
    "          doc_pg_eng,\n",
    "          'water',\n",
    "          if_exists='replace',\n",
    "          index=False)\n",
    "\n",
    "# Use ('resa_stn_id', 'sim_yr') as primary key\n",
    "sql = (\"ALTER TABLE water.magic_regression_data \"\n",
    "       \"ADD CONSTRAINT water_magic_regression_data_pk \"\n",
    "       \"PRIMARY KEY (resa_stn_id, sim_yr)\")\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Perform regression\n",
    "\n",
    "In the MAGIC data table, values for `bc_x_k` in 1860 are used to define $BC^*_0$, and values from 1986 are used to define $BC^*$. The regression uses $BC^*$ as the x-variable and $BC^*_0$ as the y-variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "sql = (\"SELECT resa_stn_id, sim_yr, bc_x_k \"\n",
    "       \"FROM water.magic_regression_data\")\n",
    "df = pd.read_sql(sql, doc_pg_eng)\n",
    "df.index = df['resa_stn_id']\n",
    "del df['resa_stn_id']\n",
    "\n",
    "# Split by year\n",
    "bc0_df = df.query('sim_yr == 1860')\n",
    "del bc0_df['sim_yr']\n",
    "bc0_df.columns = ['BC0']\n",
    "\n",
    "bc_df = df.query('sim_yr == 1986')\n",
    "del bc_df['sim_yr']\n",
    "bc_df.columns = ['BC']\n",
    "\n",
    "# Join\n",
    "df = bc0_df.join(bc_df)\n",
    "\n",
    "# Regression\n",
    "res = sm.ols(formula='BC0 ~ BC', data=df).fit()\n",
    "\n",
    "print (res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same result as in Kari's Excel spreadsheet. Note, however, that the intercept is not significantly different from zero. This implies an alternative, \"slope-only\" model *might* be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "res = sm.ols(formula='BC0 ~ BC - 1', data=df).fit()\n",
    "print (res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is better (lower AIC and BIC) than the original, and it's also simpler. I therefore propose using\n",
    "\n",
    "$$BC^*_0 = 0.9481BC^*_t$$\n",
    "\n",
    "instead of \n",
    "\n",
    "$$BC^*_0 = 0.9431BC^*_t + 0.2744$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XXXX. Create summary tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f9001172c88>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create water schema\n",
    "sql = \"CREATE SCHEMA summaries\"\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f900115c128>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create table for overall summary\n",
    "sql = (\"CREATE TABLE summaries.national_summary \"\n",
    "       \"( \"\n",
    "       \"  series_id integer NOT NULL, \"\n",
    "       \"  medium varchar NOT NULL, \"\n",
    "       \"  total_area_km2 numeric, \"\n",
    "       \"  exceeded_area_km2 numeric, \"\n",
    "       \"  exceeded_area_pct numeric, \"\n",
    "       \"  PRIMARY KEY (series_id, medium), \"\n",
    "       \"  CONSTRAINT series_id_fkey FOREIGN KEY (series_id) \"\n",
    "       \"      REFERENCES deposition.dep_series_defs (series_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION \"\n",
    "       \")\")\n",
    "doc_pg_eng.execute(sql)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
