{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import nivapy3 as nivapy\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import critical_loads as cl\n",
    "import os\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critcal Loads: DSToolkit migration (November 2019)\n",
    "\n",
    "This notebook transfers the Critical Loads data from NIVA's various internal data sources onto the JupyterHub/DSToolkit. It is modified from the workflow originally developed in [this notebook](https://nbviewer.jupyter.org/github/JamesSample/critical_loads_2/blob/master/notebooks/migrate_to_docker_test.ipynb). **The code here primarily focuses on the post-2017 workflow (\"*new method; new grid*\")**, although some older datasets are transferred as well.\n",
    "\n",
    "**Note:** This notebook must be run locally from within the NIVA firewall in order to establish a connection to the NIVABASE; it cannot be run directly from the JupyterHub.\n",
    "\n",
    "## 1. Setup \n",
    "\n",
    "### 1.1. Establish database connections\n",
    "\n",
    "Datasets are transferred from (i) the NIVABASE Oracle instance, (ii) a local PostGIS databse and (iii) various Excel, CSV and spatial datasets on NIVA's internal network. All data is stored on the JupyterHub either in a dedicated PostGIS database named `critical_loads`, or on the `shared` drive under:\n",
    "\n",
    "    shared/critical_loads\n",
    "    \n",
    "The code below installs PostGIS if it's not already present in the database, and it also grants the default `Jovyan` JupyterHub user read-only access to all the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Username:  ···\n",
      "Password:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n"
     ]
    }
   ],
   "source": [
    "# Connect to the NIVABASE\n",
    "ora_eng = nivapy.da.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Username:  ········\n",
      "Password:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n"
     ]
    }
   ],
   "source": [
    "# Connect to local PostGIS\n",
    "loc_pg_eng = nivapy.da.connect_postgis(admin=True,\n",
    "                                       host='host.docker.internal',\n",
    "                                       database='niva_work',\n",
    "                                       port=5432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Username:  ········\n",
      "Password:  ·······\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n"
     ]
    }
   ],
   "source": [
    "# Connect to Jupyter PostGIS\n",
    "jup_pg_eng = nivapy.da.connect_postgis(admin=True,\n",
    "                                       host='104.199.55.41',\n",
    "                                       database='critical_loads',\n",
    "                                       port=5432)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Define schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7fbd9cb1df28>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Activate PostGIS extension\n",
    "sql = \"CREATE EXTENSION IF NOT EXISTS postgis\"\n",
    "jup_pg_eng.execute(sql)\n",
    "\n",
    "# Create deposition schema\n",
    "sql = \"CREATE SCHEMA deposition\"\n",
    "jup_pg_eng.execute(sql)\n",
    "\n",
    "# Create veg schema\n",
    "sql = \"CREATE SCHEMA vegetation\"\n",
    "jup_pg_eng.execute(sql)\n",
    "\n",
    "# Create water schema\n",
    "sql = \"CREATE SCHEMA water\"\n",
    "jup_pg_eng.execute(sql)\n",
    "\n",
    "# Create soil schema\n",
    "sql = \"CREATE SCHEMA soil\"\n",
    "jup_pg_eng.execute(sql)\n",
    "\n",
    "# Create summary schema\n",
    "sql = \"CREATE SCHEMA summaries\"\n",
    "doc_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grant \"ReadOnly\" privileges to default Jovyan user\n",
    "sql_list = [\n",
    "    'GRANT CONNECT ON DATABASE critical_loads TO jovyan',\n",
    "    'GRANT USAGE ON SCHEMA deposition TO jovyan',\n",
    "    'GRANT USAGE ON SCHEMA vegetation TO jovyan',\n",
    "    'GRANT USAGE ON SCHEMA water TO jovyan',\n",
    "    'GRANT USAGE ON SCHEMA soil TO jovyan',\n",
    "    'GRANT USAGE ON SCHEMA summaries TO jovyan',\n",
    "    'GRANT SELECT ON ALL TABLES IN SCHEMA deposition TO jovyan',\n",
    "    'GRANT SELECT ON ALL TABLES IN SCHEMA vegetation TO jovyan',\n",
    "    'GRANT SELECT ON ALL TABLES IN SCHEMA water TO jovyan',\n",
    "    'GRANT SELECT ON ALL TABLES IN SCHEMA soil TO jovyan',\n",
    "    'GRANT SELECT ON ALL TABLES IN SCHEMA summaries TO jovyan',\n",
    "    'ALTER DEFAULT PRIVILEGES IN SCHEMA deposition GRANT SELECT ON TABLES TO jovyan',\n",
    "    'ALTER DEFAULT PRIVILEGES IN SCHEMA vegetation GRANT SELECT ON TABLES TO jovyan',\n",
    "    'ALTER DEFAULT PRIVILEGES IN SCHEMA water GRANT SELECT ON TABLES TO jovyan',\n",
    "    'ALTER DEFAULT PRIVILEGES IN SCHEMA soil GRANT SELECT ON TABLES TO jovyan',\n",
    "    'ALTER DEFAULT PRIVILEGES IN SCHEMA summaries GRANT SELECT ON TABLES TO jovyan',\n",
    "]\n",
    "\n",
    "for sql in sql_list:\n",
    "    jup_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deposition data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Transfer BLR and 0.1 degree deposition grids\n",
    "\n",
    "From 2018 onwards, deposition data from NILU is supplied on a 0.1 degree grid. The code below transfers it to the JupyterHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7fbd9ca36d68>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 0.1 degree dep grid from PostGIS\n",
    "sql = (\"SELECT * FROM public.dep_grid_0_1deg\")\n",
    "dep_gdf = gpd.read_postgis(sql, loc_pg_eng)\n",
    "\n",
    "# Write to new db\n",
    "nivapy.da.gdf_to_postgis(dep_gdf, \n",
    "                         'dep_grid_0_1deg', \n",
    "                         'deposition', \n",
    "                         jup_pg_eng,\n",
    "                         'dep_dep_grid_0_1deg_spidx',\n",
    "                         if_exists='replace',\n",
    "                         index=False,\n",
    "                         method='multi',\n",
    "                         chunksize=1000,\n",
    "                        )\n",
    "\n",
    "#  Drop primary key col added automatically by NivaPy\n",
    "sql = (\"ALTER TABLE deposition.dep_grid_0_1deg DROP COLUMN id\")\n",
    "jup_pg_eng.execute(sql)\n",
    "\n",
    "# Use 'cell_id' col as primary key\n",
    "sql = (\"ALTER TABLE deposition.dep_grid_0_1deg \"\n",
    "       \"ADD CONSTRAINT dep_grid_0_1deg_pk \"\n",
    "       \"PRIMARY KEY (cell_id)\")\n",
    "jup_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous years, data have been supplied using the BLR grid, which is available as a shapefile. The code below loads this into the database as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! The dataframe contains mixed geometries:\n",
      "  {'MultiPolygon', 'Polygon'}\n",
      "These will be cast to \"Multi\" type. If this is not what you want, consider using gdf.explode() first\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7fbd9c793c18>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read BLR .shp\n",
    "blr_path = (r'../../../data/vector/blrgrid_uten_grums.shp')\n",
    "dep_gdf = gpd.read_file(blr_path)\n",
    "dep_gdf.rename({'BLR':'blr'},\n",
    "               axis=1,\n",
    "               inplace=True)\n",
    "\n",
    "# Write to new db\n",
    "nivapy.da.gdf_to_postgis(dep_gdf, \n",
    "                         'dep_grid_blr', \n",
    "                         'deposition', \n",
    "                         jup_pg_eng,\n",
    "                         'dep_dep_grid_blr_spidx',\n",
    "                         if_exists='replace',\n",
    "                         index=False,\n",
    "                         method='multi',\n",
    "                         chunksize=1000,\n",
    "                        )\n",
    "\n",
    "#  Drop primary key col added automatically by NivaPy\n",
    "sql = (\"ALTER TABLE deposition.dep_grid_blr DROP COLUMN id\")\n",
    "jup_pg_eng.execute(sql)\n",
    "\n",
    "# Use 'blr' col as primary key\n",
    "sql = (\"ALTER TABLE deposition.dep_grid_blr \"\n",
    "       \"ADD CONSTRAINT dep_grid_blr_pk \"\n",
    "       \"PRIMARY KEY (blr)\")\n",
    "jup_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Transfer \"Deposition Series Definitions\" from the NIVABASE\n",
    "\n",
    "In RESA2, the table `DEP_SERIES_DEFINITIONS` identifies the datasets previously supplied by NILU. Each dataset has a separate row (and ID) in this table. For consistency, I will transfer this table directly and use the same dataset IDs in the new workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "      <td>Middel 2012-2016</td>\n",
       "      <td>Fordelt til BLR av NILU 2017 (Wenche Aas; old ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>Middel 2012-2016 (new)</td>\n",
       "      <td>Fordelt til BLR av NILU 2017 (Wenche Aas; new ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>28</td>\n",
       "      <td>Middel 2012-2016 (new; hi-res)</td>\n",
       "      <td>Fordelt til BLR av NILU 2017 (Wenche Aas; new ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>Middel 1983-1987</td>\n",
       "      <td>Fordelt til BLR av NILU 2019 (Wenche Aas; old ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>Middel 1988-1992</td>\n",
       "      <td>Fordelt til BLR av NILU 2019 (Wenche Aas; old ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    series_id                            name  \\\n",
       "28         26                Middel 2012-2016   \n",
       "16         27          Middel 2012-2016 (new)   \n",
       "17         28  Middel 2012-2016 (new; hi-res)   \n",
       "29         29                Middel 1983-1987   \n",
       "27         30                Middel 1988-1992   \n",
       "\n",
       "                                          description  \n",
       "28  Fordelt til BLR av NILU 2017 (Wenche Aas; old ...  \n",
       "16  Fordelt til BLR av NILU 2017 (Wenche Aas; new ...  \n",
       "17  Fordelt til BLR av NILU 2017 (Wenche Aas; new ...  \n",
       "29  Fordelt til BLR av NILU 2019 (Wenche Aas; old ...  \n",
       "27  Fordelt til BLR av NILU 2019 (Wenche Aas; old ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from RESA\n",
    "sql = \"SELECT * FROM resa2.dep_series_definitions\"\n",
    "df = pd.read_sql(sql, ora_eng)\n",
    "\n",
    "# Tidy\n",
    "df.rename({'dep_series_id':'series_id'},\n",
    "          axis=1,\n",
    "          inplace=True)\n",
    "df.sort_values('series_id', inplace=True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7fbd9c7a4668>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write to new db\n",
    "df.to_sql('dep_series_defs', \n",
    "          jup_pg_eng,\n",
    "          'deposition',\n",
    "          if_exists='replace',\n",
    "          method='multi',\n",
    "          chunksize=1000,\n",
    "          index=False,\n",
    "         )\n",
    "\n",
    "# Use 'dep_series_id' col as primary key\n",
    "sql = (\"ALTER TABLE deposition.dep_series_defs \"\n",
    "       \"ADD CONSTRAINT dep_series_defs_pk \"\n",
    "       \"PRIMARY KEY (series_id)\")\n",
    "jup_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Transfer \"Deposition Parameter Definitions\" from the NIVABASE\n",
    "\n",
    "In RESA2, the table `AIR_PARAMETER_DEFINITIONS` identifies various deposition parameters. As far as I can see, only the first three or four entries in this table are currently relevant. Many of the columns also seem unnecessary at present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_id</th>\n",
       "      <th>name</th>\n",
       "      <th>unit</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>N(oks)</td>\n",
       "      <td>mg N/m2/year</td>\n",
       "      <td>Oxidized nitrogen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>N(red)</td>\n",
       "      <td>mg N/m2/year</td>\n",
       "      <td>Reduced nitogen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>S</td>\n",
       "      <td>mg S/m2/year</td>\n",
       "      <td>Total sulphur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>S*</td>\n",
       "      <td>mg S/m2/year</td>\n",
       "      <td>Non marine sulphur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_id    name          unit          description\n",
       "0         1  N(oks)  mg N/m2/year    Oxidized nitrogen\n",
       "1         2  N(red)  mg N/m2/year      Reduced nitogen\n",
       "2         3       S  mg S/m2/year        Total sulphur\n",
       "3         4      S*  mg S/m2/year  Non marine sulphur "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from RESA\n",
    "sql = (\"SELECT * FROM resa2.air_parameter_definitions \"\n",
    "       \"WHERE parameter_id < 5\")\n",
    "df = pd.read_sql(sql, ora_eng)\n",
    "\n",
    "# Tidy\n",
    "df.sort_values('parameter_id', inplace=True)\n",
    "df.drop(['formula', 'category', 'function', 'entered_by', 'entered_date'],\n",
    "        axis=1,\n",
    "        inplace=True)\n",
    "df.rename({'parameter_id':'param_id'}, \n",
    "          inplace=True,\n",
    "          axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7fbd9ca0d7f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write to new db\n",
    "df.to_sql('dep_param_defs', \n",
    "          jup_pg_eng,\n",
    "          'deposition',\n",
    "          if_exists='replace',\n",
    "          index=False,\n",
    "          method='multi',\n",
    "          chunksize=1000,\n",
    "         )\n",
    "\n",
    "# Use 'dep_series_id' col as primary key\n",
    "sql = (\"ALTER TABLE deposition.dep_param_defs \"\n",
    "       \"ADD CONSTRAINT dep_param_defs_pk \"\n",
    "       \"PRIMARY KEY (param_id)\")\n",
    "jup_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Data for \"Deposition Values\"\n",
    "\n",
    "#### 2.4.1. Create tables\n",
    "\n",
    "The old workflow used the coarse BLR grid for deposition values. Deposition `series_ids` from 1 to 27 are all based on this grid, so these values will be transferred to a new table named `'dep_values_blr_grid'`. Deposition `series_id=28` uses the new 0.1 degree grid, so this dataset (and subsequent ones) will need adding to a separate table (`'dep_values_0_1deg_grid'`). As future data will also be delivered using this grid, it will be useful to have a function for processing and adding raw data to this table.\n",
    "\n",
    "Additionally, both these tables need constraints to ensure that relevant series and parameter IDs are defined before uploading data. Note that I have not included a constraint on the BLR/cell ID, because the data supplied by NILU often includes cells that are outside of terrestrial Norway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7fbd9ca0d438>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete if already exist\n",
    "sql = (\"DROP TABLE IF EXISTS deposition.dep_values_blr_grid, \"\n",
    "       \"  deposition.dep_values_0_1deg_grid\")\n",
    "jup_pg_eng.execute(sql)\n",
    "\n",
    "# Create table for BLR data\n",
    "sql = (\"CREATE TABLE deposition.dep_values_blr_grid \"\n",
    "       \"( \"\n",
    "       \"  series_id integer NOT NULL, \"\n",
    "       \"  blr integer NOT NULL, \"\n",
    "       \"  param_id integer NOT NULL, \"\n",
    "       \"  value numeric, \"\n",
    "       \"  PRIMARY KEY (series_id, blr, param_id), \"\n",
    "       \"  CONSTRAINT series_id_fkey FOREIGN KEY (series_id) \"\n",
    "       \"      REFERENCES deposition.dep_series_defs (series_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION, \"\n",
    "       \"  CONSTRAINT param_id_fkey FOREIGN KEY (param_id) \"\n",
    "       \"      REFERENCES deposition.dep_param_defs (param_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION \"\n",
    "       \")\")\n",
    "jup_pg_eng.execute(sql)\n",
    "\n",
    "# Create table for 0.1 degree data\n",
    "sql = (\"CREATE TABLE deposition.dep_values_0_1deg_grid \"\n",
    "       \"( \"\n",
    "       \"  series_id integer NOT NULL, \"\n",
    "       \"  cell_id integer NOT NULL, \"\n",
    "       \"  param_id integer NOT NULL, \"\n",
    "       \"  value numeric, \"\n",
    "       \"  PRIMARY KEY (series_id, cell_id, param_id), \"\n",
    "       \"  CONSTRAINT series_id_fkey FOREIGN KEY (series_id) \"\n",
    "       \"      REFERENCES deposition.dep_series_defs (series_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION, \"\n",
    "       \"  CONSTRAINT param_id_fkey FOREIGN KEY (param_id) \"\n",
    "       \"      REFERENCES deposition.dep_param_defs (param_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION \"\n",
    "       \")\")\n",
    "jup_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2. Transfer \"old\" data\n",
    "\n",
    "The old data using the BLR grid are stored in RESA2 in the table `DEP_BLR_VALUES`. We'll just transfer the data for `param_ids` 1 to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from RESA\n",
    "sql = (\"SELECT * FROM resa2.dep_blr_values \"\n",
    "       \"WHERE parameter_id < 5\")\n",
    "df = pd.read_sql(sql, ora_eng)\n",
    "\n",
    "# Tidy\n",
    "df.rename({'dep_series_id':'series_id',\n",
    "           'parameter_id':'param_id'},\n",
    "          axis=1,\n",
    "          inplace=True)\n",
    "\n",
    "# Write to new db\n",
    "df.to_sql('dep_values_blr_grid', \n",
    "          jup_pg_eng,\n",
    "          'deposition',\n",
    "          if_exists='append',\n",
    "          index=False,\n",
    "          method='multi',\n",
    "          chunksize=1000,\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3. Import \"new\" data\n",
    "\n",
    "In 2017, NILU supplied raw data for the 0.1 degree grid in `.dat` format. I previously wrote some code ([here](http://nbviewer.jupyter.org/github/JamesSample/critical_loads/blob/master/notebooks/critical_loads_workflow_new_grid.ipynb#1.1.-Upload-new-data-to-database)) to process this data. I have now generalised this and moved it into `critical_loads.py`, along with other useful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207000 new rows added successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>param_id</th>\n",
       "      <th>value</th>\n",
       "      <th>series_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>50050305</td>\n",
       "      <td>2</td>\n",
       "      <td>270.87</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>50050315</td>\n",
       "      <td>2</td>\n",
       "      <td>240.50</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>50050325</td>\n",
       "      <td>2</td>\n",
       "      <td>259.19</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>50050335</td>\n",
       "      <td>2</td>\n",
       "      <td>252.06</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>50050345</td>\n",
       "      <td>2</td>\n",
       "      <td>332.82</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cell_id param_id   value  series_id\n",
       "0  50050305        2  270.87         28\n",
       "1  50050315        2  240.50         28\n",
       "2  50050325        2  259.19         28\n",
       "3  50050335        2  252.06         28\n",
       "4  50050345        2  332.82         28"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process NILU data and add to db\n",
    "nilu_fold = r'../../../data/raw/nilu_dep/2012-2016'\n",
    "df = cl.upload_nilu_0_1deg_dep_data(nilu_fold, \n",
    "                                    jup_pg_eng,\n",
    "                                    28)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vegatation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Transfer critical loads table for vegetation\n",
    "\n",
    "The Excel file \n",
    "\n",
    "    new_workflow_nov_2018\\data\\raw\\veg_cl_classes\\sat_veg_land_use_classes.xlsx\n",
    "    \n",
    "contains critical load values for various vegetation classes. The original is in the sheet named `'EUNIS_tilGIS'`, but I have also created a tidied version (with e.g. lower case column names) in the sheet named `'eunis_lower_case'`.\n",
    "\n",
    "As above, if this table is likely to change, it should be normalised and loaded into the database more carefully. For now, the \"flat\" Excel format is convenient, but I nevertheless want to store it in the database with the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7fbd9bf38860>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Excel data\n",
    "xl_path = r'../../../data/raw/veg_cl_classes/sat_veg_land_use_classes.xlsx'\n",
    "df = pd.read_excel(xl_path, sheet_name='eunis_lower_case')\n",
    "\n",
    "# Write to new db\n",
    "df.to_sql('land_class_crit_lds', \n",
    "          jup_pg_eng,\n",
    "          'vegetation',\n",
    "          if_exists='replace',\n",
    "          index=False,\n",
    "          method='multi',\n",
    "          chunksize=1000,\n",
    "         )\n",
    "\n",
    "# Use 'norut_code' as primary key\n",
    "sql = (\"ALTER TABLE vegetation.land_class_crit_lds \"\n",
    "       \"ADD CONSTRAINT veg_land_class_crit_lds_pk \"\n",
    "       \"PRIMARY KEY (norut_code)\")\n",
    "jup_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Transfer vector vegetation data\n",
    "\n",
    "Analyses undertaken for the periods 2002-2006 and 2007-2011 used a vector vegetation dataset. From 2012-2016 onwards, this has been replaced by a more detailed raster dataset (Section 3.3, below). The code in this section adds the old vector dataset to PostGIS for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read veg .shp\n",
    "shp_path = (r'../../../data/vector/nye_2010_talegrenser_veg.shp')\n",
    "veg_gdf = gpd.read_file(shp_path)\n",
    "\n",
    "# Tidy\n",
    "del veg_gdf['ID'], veg_gdf['ID_1'], veg_gdf['GRIDCODE'], veg_gdf['EUNIS_1']\n",
    "veg_gdf.columns = [i.lower() for i in veg_gdf.columns]\n",
    "\n",
    "# Reproject to WGS84\n",
    "veg_gdf = veg_gdf.to_crs({'init':'epsg:4326'})\n",
    "\n",
    "# Write to new db\n",
    "# Use PK added by NivaPy\n",
    "nivapy.da.gdf_to_postgis(veg_gdf, \n",
    "                         'vector_veg_pre_2012', \n",
    "                         'vegetation', \n",
    "                         jup_pg_eng,\n",
    "                         'vector_veg_pre_2012_spidx',\n",
    "                         if_exists='replace',\n",
    "                         index=False,\n",
    "                         method='multi',\n",
    "                         chunksize=1000,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Transfer vegetation grids\n",
    "\n",
    "The code in Section 1.3 of [this notebook](http://nbviewer.jupyter.org/github/JamesSample/critical_loads/blob/master/notebooks/critical_loads_workflow_new_grid.ipynb#1.3.-Reclassify) converted 30 m resolution satellite vegetation data into a grid of critical loads (based on the Excel table above - Section 3.1). **Unless we update the vegetation data, this calculation does not need to be performed again**.\n",
    "\n",
    "I have experimented with storing these grids in the PostGIS database, but performance seems poor. Instead, I've added them to the JupyterHub `shared` drive here:\n",
    "\n",
    "    shared/critical_loads/raster\n",
    "   \n",
    "For the vegetation exceedance calculations, the following grids are relevant:\n",
    "\n",
    " * `sat_veg_30m_all.tif`\n",
    " * `sat_veg_60m_all.tif`\n",
    " * `sat_veg_120m_all.tif`\n",
    " * `sat_veg_30m_cr_lds_div100.tif`\n",
    " * `sat_veg_60m_cr_lds_div100.tif`\n",
    " * `blr_land_mask.tif`\n",
    " * `blr_land_mask_60m.tif`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Create tables to store exceedances\n",
    "\n",
    "Summary exceedance statistics for each grid cell are stored in the database in the following tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7fbd9b8be588>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete if already exist\n",
    "sql = (\"DROP TABLE IF EXISTS vegetation.exceedance_stats_blr_grid, \"\n",
    "       \"  vegetation.exceedance_stats_0_1deg_grid\")\n",
    "jup_pg_eng.execute(sql)\n",
    "\n",
    "# Create table for BLR data\n",
    "sql = (\"CREATE TABLE vegetation.exceedance_stats_blr_grid \"\n",
    "       \"( \"\n",
    "       \"  series_id integer NOT NULL, \"\n",
    "       \"  blr integer NOT NULL, \"\n",
    "       \"  exceeded_area_km2 numeric, \"\n",
    "       \"  total_area_km2 numeric, \"\n",
    "       \"  pct_exceeded numeric, \"\n",
    "       \"  PRIMARY KEY (series_id, blr), \"\n",
    "       \"  CONSTRAINT series_id_fkey FOREIGN KEY (series_id) \"\n",
    "       \"      REFERENCES deposition.dep_series_defs (series_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION, \"\n",
    "       \"  CONSTRAINT blr_fkey FOREIGN KEY (blr) \"\n",
    "       \"      REFERENCES deposition.dep_grid_blr (blr) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION \"\n",
    "       \")\")\n",
    "jup_pg_eng.execute(sql)\n",
    "\n",
    "# Create table for BLR data\n",
    "sql = (\"CREATE TABLE vegetation.exceedance_stats_0_1deg_grid \"\n",
    "       \"( \"\n",
    "       \"  series_id integer NOT NULL, \"\n",
    "       \"  cell_id integer NOT NULL, \"\n",
    "       \"  exceeded_area_km2 numeric, \"\n",
    "       \"  total_area_km2 numeric, \"\n",
    "       \"  pct_exceeded numeric, \"\n",
    "       \"  PRIMARY KEY (series_id, cell_id), \"\n",
    "       \"  CONSTRAINT series_id_fkey FOREIGN KEY (series_id) \"\n",
    "       \"      REFERENCES deposition.dep_series_defs (series_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION, \"\n",
    "       \"  CONSTRAINT cell_id_fkey FOREIGN KEY (cell_id) \"\n",
    "       \"      REFERENCES deposition.dep_grid_0_1deg (cell_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION \"\n",
    "       \")\")\n",
    "jup_pg_eng.execute(sql)\n",
    "\n",
    "# Create table for exceedance per land class data\n",
    "sql = (\"CREATE TABLE vegetation.exceedance_stats_land_class \"\n",
    "       \"( \"\n",
    "       \"  series_id integer NOT NULL, \"\n",
    "       \"  norut_code integer NOT NULL, \"\n",
    "       \"  exceeded_area_km2 numeric, \"\n",
    "       \"  total_area_km2 numeric, \"\n",
    "       \"  pct_exceeded numeric, \"\n",
    "       \"  PRIMARY KEY (series_id, norut_code), \"\n",
    "       \"  CONSTRAINT series_id_fkey FOREIGN KEY (series_id) \"\n",
    "       \"      REFERENCES deposition.dep_series_defs (series_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION, \"\n",
    "       \"  CONSTRAINT norut_code_fkey FOREIGN KEY (norut_code) \"\n",
    "       \"      REFERENCES vegetation.land_class_crit_lds (norut_code) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION \"\n",
    "       \")\")\n",
    "jup_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Water data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. MAGIC model output for regression\n",
    "\n",
    "Kari has a spreadsheet here\n",
    "\n",
    "    K:\\Avdeling\\317 Klima- og miljømodellering\\KAU\\Focal Centre\\Data\\bc0regresjonNOK_TL2005-rapport_KAU.xls\n",
    "    \n",
    "containing output from the MAGIC model. This is used to generate parameters for the calculation of critical loads to water. It is unclear whether this data will be updated in the future. If so, the dataset should be properly normalised before adding to the database. However, for now I'm assuming that this is a static dataset, so I'm just adding the essential data to the database as a \"flat\" table for the purposes of data storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7fbd9b8c94e0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read tidied Excel data\n",
    "xl_path = r'../../../../water/update_nov_2018/regression_data_tidied.xls'\n",
    "df = pd.read_excel(xl_path, sheet_name='tidied')\n",
    "\n",
    "# Write to new db\n",
    "df.to_sql('magic_regression_data', \n",
    "          jup_pg_eng,\n",
    "          'water',\n",
    "          if_exists='replace',\n",
    "          index=False,\n",
    "          method='multi',\n",
    "          chunksize=1000,\n",
    "         )\n",
    "\n",
    "# Use ('resa_stn_id', 'sim_yr') as primary key\n",
    "sql = (\"ALTER TABLE water.magic_regression_data \"\n",
    "       \"ADD CONSTRAINT water_magic_regression_data_pk \"\n",
    "       \"PRIMARY KEY (resa_stn_id, sim_yr)\")\n",
    "jup_pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Perform regression\n",
    "\n",
    "In the MAGIC data table, values for `bc_x_k` in 1860 are used to define $BC^*_0$, and values from 1986 are used to define $BC^*$. The regression uses $BC^*$ as the x-variable and $BC^*_0$ as the y-variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    BC0   R-squared:                       0.985\n",
      "Model:                            OLS   Adj. R-squared:                  0.985\n",
      "Method:                 Least Squares   F-statistic:                     5320.\n",
      "Date:                Mon, 25 Nov 2019   Prob (F-statistic):           1.20e-75\n",
      "Time:                        12:54:41   Log-Likelihood:                -211.76\n",
      "No. Observations:                  83   AIC:                             427.5\n",
      "Df Residuals:                      81   BIC:                             432.4\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.2744      0.536      0.512      0.610      -0.792       1.340\n",
      "BC             0.9431      0.013     72.938      0.000       0.917       0.969\n",
      "==============================================================================\n",
      "Omnibus:                       26.839   Durbin-Watson:                   1.981\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              202.031\n",
      "Skew:                          -0.530   Prob(JB):                     1.35e-44\n",
      "Kurtosis:                      10.569   Cond. No.                         64.4\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "sql = (\"SELECT resa_stn_id, sim_yr, bc_x_k \"\n",
    "       \"FROM water.magic_regression_data\")\n",
    "df = pd.read_sql(sql, jup_pg_eng)\n",
    "df.index = df['resa_stn_id']\n",
    "del df['resa_stn_id']\n",
    "\n",
    "# Split by year\n",
    "bc0_df = df.query('sim_yr == 1860')\n",
    "del bc0_df['sim_yr']\n",
    "bc0_df.columns = ['BC0']\n",
    "\n",
    "bc_df = df.query('sim_yr == 1986')\n",
    "del bc_df['sim_yr']\n",
    "bc_df.columns = ['BC']\n",
    "\n",
    "# Join\n",
    "df = bc0_df.join(bc_df)\n",
    "\n",
    "# Regression\n",
    "res = sm.ols(formula='BC0 ~ BC', data=df).fit()\n",
    "\n",
    "print (res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same result as in Kari's Excel spreadsheet. Note, however, that the intercept is not significantly different from zero. This implies an alternative, \"slope-only\" model *might* be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                    BC0   R-squared (uncentered):                   0.994\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.994\n",
      "Method:                 Least Squares   F-statistic:                          1.310e+04\n",
      "Date:                Mon, 25 Nov 2019   Prob (F-statistic):                    3.12e-92\n",
      "Time:                        12:54:58   Log-Likelihood:                         -211.89\n",
      "No. Observations:                  83   AIC:                                      425.8\n",
      "Df Residuals:                      82   BIC:                                      428.2\n",
      "Df Model:                           1                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "BC             0.9481      0.008    114.449      0.000       0.932       0.965\n",
      "==============================================================================\n",
      "Omnibus:                       28.577   Durbin-Watson:                   1.977\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              189.981\n",
      "Skew:                          -0.682   Prob(JB):                     5.58e-42\n",
      "Kurtosis:                      10.285   Cond. No.                         1.00\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Regression\n",
    "res = sm.ols(formula='BC0 ~ BC - 1', data=df).fit()\n",
    "print (res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is better (lower AIC and BIC) than the original, and it's also simpler. I therefore propose using\n",
    "\n",
    "$$BC^*_0 = 0.9481BC^*_t$$\n",
    "\n",
    "instead of \n",
    "\n",
    "$$BC^*_0 = 0.9431BC^*_t + 0.2744$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Soils data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Critical Load for S\n",
    "\n",
    "Critical loads for soil may be updated in the future, but for now I'll transfer the values for S stored in RESA2.TALRGEN_VALUES (with a unit conversion to $mgS/m^2/yr$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete if already exist\n",
    "sql = (\"DROP TABLE IF EXISTS soil.s_critical_load\")\n",
    "jup_pg_eng.execute(sql)\n",
    "\n",
    "# Create table for BLR data\n",
    "sql = (\"CREATE TABLE soil.s_critical_load \"\n",
    "       \"( \"\n",
    "       \"  blr integer PRIMARY KEY, \"\n",
    "       \"  cl_mgSpm2 numeric, \"\n",
    "       \"  total_area_km2 numeric, \"\n",
    "       \"  pct_exceeded numeric, \"\n",
    "       \"  PRIMARY KEY (series_id, blr), \"\n",
    "       \"  CONSTRAINT blr_fkey FOREIGN KEY (blr) \"\n",
    "       \"      REFERENCES deposition.dep_grid_blr (blr) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION \"\n",
    "       \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all CLs for S\n",
    "# in meq/m2/year\n",
    "sql = (\"SELECT blr, xvalue as crit_ld \"\n",
    "       \"FROM resa2.talegren_values \"\n",
    "       \"WHERE talegren_paramid = 86\")\n",
    "\n",
    "cl_df = pd.read_sql(sql, ora_eng)\n",
    "cl_df.index = cl_df['blr']\n",
    "del cl_df['blr']\n",
    "\n",
    "# Convert to mg-S/m2/yr\n",
    "cl_df['cl_mgSpm2'] = cl_df['crit_ld']*32.06 / 2.\n",
    "del cl_df['crit_ld']\n",
    "\n",
    "# Remove negative CL\n",
    "cl_df = cl_df.query('cl_mgSpm2 >= 0')\n",
    "cl_df.reset_index(inplace=True)\n",
    "\n",
    "# Write to db\n",
    "df.to_sql('s_critical_load', \n",
    "          jup_pg_eng,\n",
    "          'soil',\n",
    "          if_exists='append',\n",
    "          index=False,\n",
    "          method='multi',\n",
    "          chunksize=1000,\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create summary tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table for overall summary\n",
    "sql = (\"CREATE TABLE summaries.national_summary \"\n",
    "       \"( \"\n",
    "       \"  series_id integer NOT NULL, \"\n",
    "       \"  medium varchar NOT NULL, \"\n",
    "       \"  total_area_km2 numeric, \"\n",
    "       \"  exceeded_area_km2 numeric, \"\n",
    "       \"  exceeded_area_pct numeric, \"\n",
    "       \"  PRIMARY KEY (series_id, medium), \"\n",
    "       \"  CONSTRAINT series_id_fkey FOREIGN KEY (series_id) \"\n",
    "       \"      REFERENCES deposition.dep_series_defs (series_id) \"\n",
    "       \"      ON UPDATE NO ACTION ON DELETE NO ACTION \"\n",
    "       \")\")\n",
    "doc_pg_eng.execute(sql)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
